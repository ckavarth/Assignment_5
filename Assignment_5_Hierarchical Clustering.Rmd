---
title: "Assignment_5_Hierarchical Clustering"
author: "Chaitanya"
date: "2024-04-07"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Hierarchical clustering is a type of clustering algorithm that creates a hierarchy of clusters. It starts by treating each data point as its own cluster and then successively merges or splits clusters based on a specified distance metric until all data points belong to a single cluster.two main types of hierarchical clustering:1)Agglomerative (bottom-up) clustering has Different linkage methods (such as single linkage, complete linkage, average linkage, and Ward's method)& 2) Divisive (top-down) clustering:


```{r}
# Load necessary libraries
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(knitr)
library(readr)
```


```{r}
# Read the dataset
Cereals_data <- read.csv("C:\\Users\\Chaitu\\Documents\\Cereals.csv")
Numerical_data <- data.frame(Cereals_data[,4:16])
```


```{r}
# Remove cereals with missing values
Numerical_data <- na.omit(Numerical_data)
```


```{r}
#Normalizing data
norm_cereals<- scale(Numerical_data)
```


```{r}
#By Using the normalized data to do hierarchical clustering using the Euclidean Dist technique.
EuclideanDist <- dist(norm_cereals, method = "euclidean")
Hierarchial_clust <- hclust(EuclideanDist, method = "complete")
```


```{r}
# Hierarchical clustering produces a dendrogram, which is a tree-like diagram that illustrates the arrangement of the clusters.
plot(Hierarchial_clust, cex = 0.7, hang = -1)
```


```{r}
# Apply hierarchical clustering using different methods
single_Hierarchial_clust <- agnes(norm_cereals, method = "single")
complete_Hierarchial_clust <- agnes(norm_cereals, method = "complete")
average_Hierarchial_clust <- agnes(norm_cereals, method = "average")
ward_Hierarchial_clust <- agnes(norm_cereals, method = "ward")
```


```{r}
# Here printing the Agglomerative (bottom-up) clustering:
cat("Single Hierarchial_clust:", single_Hierarchial_clust$ac, "\n")
cat("Complete Hierarchial_clust:", complete_Hierarchial_clust$ac, "\n")
cat("Average Hierarchial_clust:", average_Hierarchial_clust$ac, "\n")
cat("Ward's Method:", ward_Hierarchial_clust$ac, "\n")
```
# From the above hierarchial clustering the ward's method is highest, because the value is 0.9046042 comparing to others.

```{r}
#2. Choosing the clusters
pltree(ward_Hierarchial_clust, cex = 0.5, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(ward_Hierarchial_clust, k = 5, border = 2:7)
```


```{r}
Struc_Group <- cutree(ward_Hierarchial_clust, k=5)
Dataframe_2 <- as.data.frame(cbind(norm_cereals,Struc_Group))
fviz_cluster(list(data = Dataframe_2, cluster = Struc_Group))

```
#From the Cluster plot observation mentioned above, clusters can be selected. 
#determining the stability and structure ofthe clusters.
```{r}
#Building Partitions
set.seed(123)
First_partition <- Numerical_data[1:50,]
Second_partition <- Numerical_data[51:74,]
```

```{r}
#Performing Hierarchical Clustering while considering k = 5.
single_stability <- agnes(scale(First_partition), method = "single")
complete_stability <- agnes(scale(First_partition), method = "complete")
average_stability <- agnes(scale(First_partition), method = "average")
ward_stability <- agnes(scale(First_partition), method = "ward")
cbind(single=single_stability$ac , complete=complete_stability$ac , average= average_stability$ac , ward= ward_stability$ac)
```
#Plotting for ward's method for Dendogram of Agnes with Partitioned Data.
```{r}
pltree(ward_stability, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(ward_stability, k = 5, border = 2:7)
```

```{r}
clust_2 <- cutree(ward_stability, k = 5)

#the centroids are calculated.
Stability_result <- as.data.frame(cbind(First_partition, clust_2))
Stability_result[Stability_result$clust_2==1,]

```

#For first centroid
```{r}
First_centroid <- colMeans(Stability_result[Stability_result$clust_2==1,])
Stability_result[Stability_result$clust_2==2,]
```
#Printing the Second centroid
```{r}
Second_centroid <- colMeans(Stability_result[Stability_result$clust_2==2,])
Stability_result[Stability_result$clust_2==3,]
```

#Printing the third_centroid
```{r}
third_centroid <- colMeans(Stability_result[Stability_result$clust_2==3,])
Stability_result[Stability_result$clust_2==4,]

```

#Printing the fourth_centroid
```{r}
fourth_centroid <- colMeans(Stability_result[Stability_result$clust_2==4,])
centroids <- rbind(First_centroid, Second_centroid, third_centroid, fourth_centroid)
x2 <- as.data.frame(rbind(centroids[,-14], Second_partition))

```

```{r}
#figuring out the Distance
Distance_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Distance_1)
dataframe1 <- data.frame(data=seq(1,nrow(Second_partition),1), Clusters = rep(0,nrow(Second_partition)))
for(i in 1:nrow(Second_partition))
{dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1
```

```{r}
cbind(Dataframe_2$Struc_Group[51:74], dataframe1$Clusters)
```

```{r}
table(Dataframe_2$Struc_Group[51:74] == dataframe1$Clusters)

```
#Here False & True 12 denotes that the model is partially unstable.

#The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?
```{r}
#Clustering Healthy SB_Cereals.
Healthy_Cereals <- Cereals_data
Healthy_Cereals_RE <- na.omit(Healthy_Cereals)
clust <- cbind(Healthy_Cereals_RE, Struc_Group)
clust[clust$Struc_Group==1,]

```

```{r}
clust[clust$Struc_Group==2,]

```
```{r}
clust[clust$Struc_Group==3,]
```

```{r}
clust[clust$Struc_Group==4,]
```

```{r}
#Mean value are used to select the best cluster for healthy diet.
mean(clust[clust$Struc_Group==1,"rating"])
```
```{r}
mean(clust[clust$Struc_Group==2,"rating"])
```

```{r}
mean(clust[clust$Struc_Group==3,"rating"])
```
```{r}
mean(clust[clust$Struc_Group==4,"rating"])

```
# From my point here cluster 1 is the highest among all struc_groups. Therefore Struc_group_1 is considered as healthy diet cluster
 